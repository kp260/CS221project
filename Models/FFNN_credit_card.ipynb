{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATASETS\n",
    "\n",
    "path = \"./Data/Taiwan_credit_card.csv\"\n",
    "data_set = pd.read_csv(path,header=0)\n",
    "#data_set = data_set.sample(frac = 1).reset_index(drop=True)\n",
    "data_set['Default'] = data_set['Default'].apply(pd.to_numeric, downcast = 'integer', errors = 'coerce')\n",
    "\n",
    "\n",
    "# Normalize columns to [0,1] range\n",
    "cols_to_norm = ['LIMIT_BAL','SEX','EDUCATION','MARRIAGE','AGE','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'\n",
    ", 'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',                                           \n",
    "                'PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "data_set[cols_to_norm] = data_set[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "train_df, test_df = train_test_split(data_set, test_size=0.2, random_state=2018, shuffle=True )\n",
    "\n",
    "\n",
    "# Split the data\n",
    "#X_test = data_set.loc[0:5999,'ID':'PAY_AMT6']\n",
    "X_test = test_df.loc[:,'ID':'PAY_AMT6']\n",
    "X_train = train_df.loc[:,'ID':'PAY_AMT6']\n",
    "Y_test = test_df.loc[:,'Default']\n",
    "Y_train = train_df.loc[:,'Default']\n",
    "X_test = pd.DataFrame.as_matrix(X_test)\n",
    "X_train = pd.DataFrame.as_matrix(X_train)\n",
    "Y_test = pd.DataFrame.as_matrix(Y_test)\n",
    "Y_train = pd.DataFrame.as_matrix(Y_train)\n",
    "Y_test = np.reshape(Y_test, [Y_test.shape[0],1])\n",
    "Y_train = np.reshape(Y_train, [Y_train.shape[0], 1])\n",
    "\n",
    "\n",
    "# Export datasets\n",
    "np.savetxt(\"./Data/X_train.csv\", X_train, delimiter=\",\")\n",
    "np.savetxt(\"./Data/Y_train.csv\", Y_train, delimiter=\",\")\n",
    "np.savetxt(\"./Data/X_test.csv\", X_test, delimiter=\",\")\n",
    "np.savetxt(\"./Data/Y_test.csv\", Y_test, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "path_X_train = \"./Data/X_train.csv\"\n",
    "X_train = pd.read_csv(path_X_train,header=0)\n",
    "path_Y_train = \"./Data/Y_train.csv\"\n",
    "Y_train = pd.read_csv(path_Y_train,header=0)\n",
    "\n",
    "path_X_test = \"./Data/X_test.csv\"\n",
    "X_test = pd.read_csv(path_X_test,header=0)\n",
    "path_Y_test = \"./Data/Y_test.csv\"\n",
    "Y_test = pd.read_csv(path_Y_test,header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame.as_matrix(X_test)\n",
    "X_train = pd.DataFrame.as_matrix(X_train)\n",
    "Y_test = pd.DataFrame.as_matrix(Y_test)\n",
    "Y_train = pd.DataFrame.as_matrix(Y_train)\n",
    "Y_test = np.reshape(Y_test, [Y_test.shape[0],1])\n",
    "Y_train = np.reshape(Y_train, [Y_train.shape[0], 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, number of features\n",
    "    n_y -- scalar, number of classes ({0,1}-->2)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\" ?int\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_x))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1,layer_dims[l])\n",
    "    \"\"\"\n",
    "     \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable('W' + str(l), [layer_dims[l-1], layer_dims[l]], initializer = tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "        parameters['b' + str(l)] = tf.get_variable('b' + str(l), [1, layer_dims[l]], initializer = tf.zeros_initializer(), dtype=tf.float32)\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (1,layer_dims[l]))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, rate):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (n_x, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ...\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "            \n",
    "    Z1 = tf.add(tf.matmul(X, W1),b1)\n",
    "    A1 = tf.nn.dropout(tf.nn.relu(Z1), rate = rate[0])      \n",
    "    \n",
    "    Z2 = tf.add(tf.matmul(A1,W2),b2)\n",
    "    A2 = tf.nn.dropout(tf.nn.relu(Z2), rate = rate[1])\n",
    "                                             \n",
    "    Z3 = tf.add(tf.matmul(A2, W3), b3) \n",
    "    A3 = tf.nn.dropout(tf.nn.relu(Z3) , rate = rate[2])\n",
    "                                          \n",
    "    Z4 = tf.add(tf.matmul(A3, W4), b4)                                             \n",
    "   \n",
    "    \n",
    "    return Z4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z4, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z1 -- output of forward propagation (output of the LINEAR unit), of shape (n_y, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z1\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "   \n",
    "    logits = Z4\n",
    "    labels = Y\n",
    "    \n",
    "  \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:]\n",
    "    shuffled_Y = Y[permutation,:].reshape((m, Y.shape[1]))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001,\n",
    "          num_epochs = 100, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a multi-layer tensorflow neural network: [LINEAR->RELU]x -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = n_x, number of training examples = train_m)\n",
    "    Y_train -- test set, of shape (output size = n_y, number of training examples = train_m)\n",
    "    X_test -- training set, of shape (input size = n_x, number of training examples = test_m)\n",
    "    Y_test -- test set, of shape (output size = n_y, number of test examples = test_m)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (m, n_x) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[1]                            # n_y : output size\n",
    "    rate_test=[0.0, 0.0, 0.0]\n",
    "    rate_train = [1-0.35, 0.5, 1-0.75]\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    rate = tf.placeholder(tf.float32)\n",
    "   \n",
    "\n",
    "    # Initialize parameters\n",
    "    layer_dims = [n_x, 100, 50, 10, n_y]\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z4 = forward_propagation(X, parameters, rate) \n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z4, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    gvs = optimizer.compute_gradients(cost)\n",
    "    train_op = optimizer.apply_gradients(gvs)\n",
    "   \n",
    "   \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)     \n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            \n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            count=0\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                count+=1\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                             \n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([train_op, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, rate:rate_train})\n",
    "                \n",
    "                \n",
    "                \n",
    "                #print (\"count, minibatch_cost:\", count, minibatch_cost)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        raw_prediction = tf.sigmoid(Z4)\n",
    "        prediction = raw_prediction >0.5\n",
    "        correct_prediction = tf.equal(tf.cast(prediction, tf.int32), tf.cast(Y, tf.int32))\n",
    "         \n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, rate:rate_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, rate:rate_test}))\n",
    "    \n",
    "        return parameters, raw_prediction.eval({X: X_test, Y: Y_test, rate:rate_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters, test_predictions = model(X_train[:,1:] , Y_train, X_test[:,1:], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PREDICTIONS\n",
    "\n",
    "np.savetxt('test_nn', test_predictions, delimiter=',')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(Y_test, classes=[0, 1])\n",
    "y_score = test_predictions\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFUSION MATRIX\n",
    "\n",
    "ypreds = test_predictions\n",
    "ypreds[ypreds<=0.5]=0\n",
    "ypreds[ypreds>0.5]=1\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, ypreds)\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, ypreds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRECISION\n",
    "\n",
    "tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALL\n",
    "\n",
    "tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY\n",
    "\n",
    "(tp+tn)/(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC, AUC, GINI\n",
    "\n",
    "plt.figure()\n",
    "lw = 1\n",
    "plt.plot(fpr[0], tpr[0], color='navy',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='-', label='Random guess benchmark')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.text(0.25, 0.5, 'AUC = %0.3f\\nGini = %0.3f' % (roc_auc[0], 2*roc_auc[0]-1),color='navy')\n",
    "#plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC\n",
    "\n",
    "roc_auc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OCCLUSION TEST #4526, 641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP AND TN EXAMPLES\n",
    "\n",
    "X_test[5736, :] # X_test[5736, :]--> TP, X_test[4669, :] -->  TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN VALUES TO REPLACE EACH FEATURE IN TURN WITH\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "xm = np.mean(X, axis = 0)\n",
    "xm = np.reshape(xm, [1, 24])\n",
    "xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED SIGMOID SCORE FOR EACH FEATURE\n",
    "\n",
    "z=[]\n",
    "count=0\n",
    "\n",
    "for i in range(1, 24):\n",
    "    x = list(X_test[641, :])\n",
    "    X = np.reshape(x, [1,24])\n",
    "    a = xm[0, i]\n",
    "    X[0, i] = a\n",
    "    \n",
    "    X = tf.cast(X[:, 1:], tf.float32)    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "            \n",
    "    Z1 = tf.add(tf.matmul(X, W1),b1)                                          \n",
    "    A1 = tf.nn.relu(Z1)                                            \n",
    "    Z2 = tf.add(tf.matmul(A1,W2),b2)\n",
    "    A2 = tf.nn.relu(Z2)                                           \n",
    "    Z3 = tf.add(tf.matmul(A2, W3), b3) \n",
    "    A3 = tf.nn.relu(Z3)                                        \n",
    "    Z4 = tf.add(tf.matmul(A3, W4), b4) \n",
    "    A4 = tf.sigmoid(Z4)\n",
    "    z.append(A4)\n",
    "    count=count+1\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    score = sess.run(z)\n",
    "    flat_list = [item for sublist in score for item in sublist]\n",
    "    flat_list = [item for sublist in flat_list for item in sublist]\n",
    "    print(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE TP AND TN EXAMPLES ON THE HEATMAP\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame({\"TN [0.227]\": flat_list},\n",
    "                  index=[\"LIMIT_BAL\", \"GENDER\", \"EDUCATION\", \"MARITAL_STATUS\", \"AGE\", \"PAY_1\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \n",
    "\"BILL_AMT1\", \"BILL_AMT2\",\"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \n",
    "\"PAY_AMT5\", \"PAY_AMT6\"])\n",
    "plt.subplots(figsize=(20,2))\n",
    "sns.heatmap(df.T, annot=True,annot_kws={\"size\":10}, fmt=\".3f\", cmap='Blues', linewidth=2, linecolor='white', cbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MEAN VALUES TO REPLACE EACH FEATURE IN TURN WITH\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "xm = np.mean(X, axis = 0)\n",
    "xm = np.reshape(xm, [1, 24])\n",
    "\n",
    "z=[]\n",
    "count=0\n",
    "\n",
    "for i in range(1, 24):\n",
    "    a = xm[0, i]\n",
    "    X = X_test\n",
    "    X[0, i] = a\n",
    "    \n",
    "    X = tf.cast(X[:, 1:], tf.float32)    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "            \n",
    "    Z1 = tf.add(tf.matmul(X, W1),b1)                                          \n",
    "    A1 = tf.nn.relu(Z1)                                            \n",
    "    Z2 = tf.add(tf.matmul(A1,W2),b2)\n",
    "    A2 = tf.nn.relu(Z2)                                           \n",
    "    Z3 = tf.add(tf.matmul(A2, W3), b3) \n",
    "    A3 = tf.nn.relu(Z3)                                        \n",
    "    Z4 = tf.add(tf.matmul(A3, W4), b4) \n",
    "    A4 = tf.sigmoid(Z4)\n",
    "    z.append(A4)\n",
    "    count=count+1\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    score = sess.run(z)\n",
    "    flat_list = [item for sublist in score for item in sublist]\n",
    "    flat_list = [item for sublist in flat_list for item in sublist]\n",
    "    print(len(flat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ADD IDs to MODIFIED SIGMOID SCORES FOR EACH FEATURE\n",
    "\n",
    "new_var = np.reshape(flat_list, [6000, 23])\n",
    "X = X_test\n",
    "new_array = np.concatenate([np.reshape(X[:,0], [6000,1]), new_var], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE MODIFIED QII = SIGMOID_ORIGINAL - MODIFIED SIGMOID\n",
    "\n",
    "p = new_array[:,1:] - test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGN CLUSTERS, K=3\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(p)\n",
    "y_kmeans = kmeans.predict(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CLUSTERS FOR 2 FEATURES\n",
    "\n",
    "plt.scatter(p[:, 6], p[:, 10], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 1], centers[:, 8], c='red', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBOW METHOD\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(p)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE on DATASET\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "comp_array = np.concatenate([p, np.reshape(y_kmeans, [6000,1])], axis=1 )\n",
    "tsneData = TSNE(2).fit_transform(comp_array[:, :23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT RESULTS\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 5\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsneData[:,0],\n",
    "                     tsneData[:,1],\n",
    "                     c = 'g',\n",
    "                     cmap=plt.cm.PuOr,\n",
    "                     s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOURCODE BY CLUSTER ASSIGNMENT\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colvals = [dt for dt in comp_array[:, -1]]\n",
    "minima = min(colvals)\n",
    "maxima = max(colvals)\n",
    "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "mapper = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\n",
    "mycolors = [mapper.to_rgba(v) for v in colvals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15, 15\n",
    "fig, ax = plt.subplots()\n",
    "ind_0 = np.where(comp_array[:,-1]==0)\n",
    "ind_1 = np.where(comp_array[:,-1]==1)\n",
    "ind_2 = np.where(comp_array[:,-1]==2)\n",
    "\n",
    "ax.scatter(tsneData[ind_0,0],\n",
    "                    tsneData[ind_0,1],\n",
    "                     c = 'b',\n",
    "                     cmap=plt.cm.PuOr,\n",
    "                     s=40)\n",
    "\n",
    "ax.scatter(tsneData[ind_1,0],\n",
    "                    tsneData[ind_1,1],\n",
    "                     c = 'g',\n",
    "                     cmap=plt.cm.PuOr,\n",
    "                     s=40)\n",
    "\n",
    "ax.scatter(tsneData[ind_2,0],\n",
    "                    tsneData[ind_2,1],\n",
    "                     c = 'r',\n",
    "                     cmap=plt.cm.PuOr,\n",
    "                     s=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final  =np.concatenate([np.reshape(new_array[:,0],[6000,1]), comp_array], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_0= final[np.where(final[:,24]==0),0]\n",
    "final_1= final[np.where(final[:,24]==1),0]\n",
    "final_2= final[np.where(final[:,24]==2),0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./Data/fianl0.csv\", final_0, delimiter=\",\")\n",
    "np.savetxt(\"./Data/fianl1.csv\", final_1, delimiter=\",\")\n",
    "np.savetxt(\"./Data/fianl2.csv\", final_2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_0 = final[np.where(final[:,24]==0),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./Data/av0.csv\", final, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
